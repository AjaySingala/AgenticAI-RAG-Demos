{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPAuxHlF4fvLlD+BELOQKtr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Agentic RAG (Retrieval-Augmented Generation) system with LangChain and LangGraph."],"metadata":{"id":"zkLTC1B6CA6D"}},{"cell_type":"markdown","source":["### 1. Setup and Imports\n","First, install the necessary libraries. This example assumes you have an API key for a model like OpenAI or Anthropic (LangChain supports many)."],"metadata":{"id":"kTuUSIBvCMNn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0--DAdN3B-B8"},"outputs":[],"source":["%pip install -qU langchain langchain-openai langgraph langchain-community faiss-cpu\n","%pip install pydantic==2.12.3\n"]},{"cell_type":"code","source":["import os\n","from langchain_openai import ChatOpenAI\n","from langchain.tools import tool\n","from langchain_core.messages import HumanMessage\n","\n","# OLD/Incorrect Import (Causes the error)\n","#from langchain.vectorstores import FAISS\n","# NEW/Correct Import\n","from langchain_community.vectorstores import FAISS\n","\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_core.documents import Document\n","from langgraph.graph import StateGraph, END\n","from typing import TypedDict, Annotated, List\n","import operator"],"metadata":{"id":"vRXNpKrdB__l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 1. Set up Environment and LLM (Replace with your actual key and model) ---\n","# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n","\n","# OpenAI API Key.\n","\n","# For Google Colab environment.\n","from google.colab import userdata\n","key = userdata.get('OPENAI_API_KEY')\n","os.environ[\"OPENAI_API_KEY\"] = key\n","\n","# For local environment.\n","#import os\n","#\n","#key = os.getenv(\"OPENAI_API_KEY\")\n","\n","if not key:\n","    raise ValueError(\"API key not found. Please set the MY_API_KEY environment variable.\")\n","\n","print(\"API Key loaded successfully!\")\n","\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"],"metadata":{"id":"X3IcISvKB_86"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Define the Agent State (LangGraph)\n","The agent's state dictates what information is passed between different steps (nodes) of the workflow."],"metadata":{"id":"BlU0fx8ACWOZ"}},{"cell_type":"code","source":["class AgentState(TypedDict):\n","    \"\"\"Represents the state of our graph.\"\"\"\n","    query: str  # The original user question\n","    context: Annotated[List[str], operator.add]  # List of retrieved document content\n","    answer: str # The final generated answer\n","    # A simple flag to control routing: True if documents were retrieved\n","    needs_rag: bool"],"metadata":{"id":"spm1nkkeB_6T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Define the Retrieval Tool\n","This function sets up a simple in-memory RAG retriever (using FAISS) and wraps it as a tool the agent can use."],"metadata":{"id":"qVtXiIqBCacR"}},{"cell_type":"code","source":["# --- Mock Data and Retriever Setup ---\n","docs = [\n","    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\"),\n","    Document(page_content=\"RAG stands for Retrieval-Augmented Generation, combining retrieval with LLMs.\"),\n","    Document(page_content=\"LangGraph allows creating stateful, multi-step agentic workflows.\"),\n","    Document(page_content=\"Agentic systems involve an LLM reasoning and deciding which tools to use.\"),\n","]\n","\n","vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())\n","retriever = vectorstore.as_retriever()\n","\n","@tool\n","def retrieve_docs(query: str) -> List[str]:\n","    \"\"\"Search the internal vector store for documents relevant to the query.\"\"\"\n","    retrieved_docs = retriever.invoke(query)\n","    return [doc.page_content for doc in retrieved_docs]\n","\n","# The agent will only have access to this tool\n","tools = [retrieve_docs]"],"metadata":{"id":"DnIzcLElB_3x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Define Graph Nodes (Functions)\n","These functions represent the steps (nodes) in the agent's workflow."],"metadata":{"id":"VIt22F95Cfi4"}},{"cell_type":"code","source":["def check_knowledge(state: AgentState) -> str:\n","    \"\"\"Node 1: Decide if RAG is needed by asking the LLM.\"\"\"\n","    query = state[\"query\"]\n","\n","    # Simple check prompt\n","    prompt = f\"\"\"\n","    You are a router agent. Your task is to determine if the following user query\n","    requires searching the internal knowledge base (RAG) or if you can answer it\n","    directly using your general knowledge.\n","\n","    Query: \"{query}\"\n","\n","    Respond with ONLY 'RAG' if the question is likely about LangChain, RAG,\n","    LangGraph, or Agentic systems. Otherwise, respond with ONLY 'DIRECT'.\n","    \"\"\"\n","\n","    # LLM call to route the query\n","    response = llm.invoke(prompt)\n","    decision = response.content.strip().upper()\n","\n","    print(f\"--- ROUTER DECISION: {decision} ---\")\n","\n","    # Update the state for the next conditional edge\n","    if decision == 'RAG':\n","        return \"call_retriever\"\n","    else:\n","        return \"generate_direct_answer\"\n","\n","def call_retriever_node(state: AgentState) -> AgentState:\n","    \"\"\"Node 2: Call the retrieve_docs tool.\"\"\"\n","    print(\"--- CALLING RETRIEVER ---\")\n","    query = state[\"query\"]\n","    # Manually calling the tool for this simple agent\n","    retrieved_context = retrieve_docs.invoke(query)\n","\n","    return {\"context\": retrieved_context, \"query\": query}\n","\n","def generate_answer(state: AgentState) -> AgentState:\n","    \"\"\"Node 3: Generate the final answer using context (if available).\"\"\"\n","    print(\"--- GENERATING FINAL ANSWER ---\")\n","    query = state[\"query\"]\n","    context = \"\\n\".join(state[\"context\"]) if state[\"context\"] else \"No specific context retrieved.\"\n","\n","    final_prompt = f\"\"\"\n","    You are an expert Q&A system. Answer the user's question based ONLY\n","    on the provided context, if it is relevant. If the context is not\n","    relevant or the question is about general knowledge, answer based on\n","    your own knowledge.\n","\n","    Context:\n","    ---\n","    {context}\n","    ---\n","\n","    Question: {query}\n","    \"\"\"\n","\n","    response = llm.invoke(final_prompt)\n","\n","    return {\"answer\": response.content, \"query\": query}"],"metadata":{"id":"3zKiN6hCB_07"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. Build and Run the LangGraph\n","The graph defines the flow and decision points for the agent."],"metadata":{"id":"8_MMQJEdCkIg"}},{"cell_type":"code","source":["# --- Build the Graph ---\n","workflow = StateGraph(AgentState)\n","\n","# Define the nodes\n","workflow.add_node(\"router\", check_knowledge)\n","workflow.add_node(\"retrieve\", call_retriever_node)\n","workflow.add_node(\"generate\", generate_answer)\n","\n","# Set the start node\n","workflow.set_entry_point(\"router\")\n","\n","# Conditional edge from router\n","# If 'RAG' is returned, go to 'retrieve'; otherwise, go to 'generate'\n","workflow.add_conditional_edges(\n","    \"router\",\n","    lambda state: state[\"needs_rag\"], # This is a placeholder, a real implementation would use the check_knowledge decision\n","    {\n","        \"call_retriever\": \"retrieve\",\n","        \"generate_direct_answer\": \"generate\"\n","    }\n",")\n","\n","# RAG path: Retrieve -> Generate -> END\n","workflow.add_edge(\"retrieve\", \"generate\")\n","\n","# Both paths end at the final answer generation\n","workflow.add_edge(\"generate\", END)\n","\n","# Compile the graph\n","# NOTE: The conditional edge logic needs refinement to correctly use the\n","# decision from `check_knowledge` as an explicit conditional branch.\n","# For simplicity, we manually run the flow below to demonstrate the components.\n","\n","# --- Example Run (Simplified Chain Execution) ---\n","def run_agentic_rag(query: str):\n","    # For a simple script, we can run the steps manually based on the router's output\n","    initial_state = {\"query\": query, \"context\": [], \"answer\": \"\", \"needs_rag\": False}\n","\n","    # Step 1: Route\n","    decision = check_knowledge(initial_state)\n","\n","    if decision == \"call_retriever\":\n","        # Step 2: Retrieve\n","        retrieved_state = call_retriever_node(initial_state)\n","        # Step 3: Generate\n","        final_state = generate_answer(retrieved_state)\n","    else:\n","        # Step 2: Generate directly\n","        final_state = generate_answer(initial_state)\n","\n","    return final_state[\"answer\"]\n","\n","# --- Example Queries ---\n","query_rag = \"What is RAG?\"\n","query_direct = \"What is the capital of France?\"\n","\n","print(f\"\\nQUERY 1: {query_rag}\")\n","result_rag = run_agentic_rag(query_rag)\n","print(f\"\\nAGENT RESPONSE: {result_rag}\")\n","\n","print(f\"\\nQUERY 2: {query_direct}\")\n","result_direct = run_agentic_rag(query_direct)\n","print(f\"\\nAGENT RESPONSE: {result_direct}\")"],"metadata":{"id":"WTcNhkuqB_yV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YuNzUnRdB_lC"},"execution_count":null,"outputs":[]}]}