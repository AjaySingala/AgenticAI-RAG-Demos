{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOy1QdA68kshncJ0Y5ZlPbE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BDSWAsVWHPE3"},"outputs":[],"source":["%pip install \\\n","    langchain \\\n","    langchain-openai \\\n","    langchain-community \\\n","    faiss-cpu \\\n","    tiktoken"]},{"cell_type":"code","source":["%pip install pypdf"],"metadata":{"id":"g21PkMo9Nzxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from typing import List\n","\n","from langchain_community.document_loaders import DirectoryLoader, TextLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_core.vectorstores import InMemoryVectorStore\n","from langchain.tools import tool\n","from langchain.chat_models import init_chat_model\n","from langchain.agents import create_agent"],"metadata":{"id":"5DihEkOLJBxL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For Google Colab environment.\n","from google.colab import userdata\n","key = userdata.get('OPENAI_API_KEY')\n","os.environ[\"OPENAI_API_KEY\"] = key\n","\n","# For local environment.\n","#import os\n","#\n","#key = os.getenv(\"OPENAI_API_KEY\")"],"metadata":{"id":"ntSB3uFHHRlB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# 1. INDEXING: load → split → store\n","# Only for .txt files.\n","# -------------------------\n","\n","#def load_local_documents(data_path: str = \"./data\"):\n","#    \"\"\"\n","#    Load all .txt files from the given folder as LangChain Documents.\n","#    Extend glob pattern or add more loaders for PDFs, etc.\n","#    \"\"\"\n","#    loader = DirectoryLoader(\n","#        data_path,\n","#        glob=\"**/*.txt\",        # change to e.g. \"**/*\" and add filters if needed\n","#        loader_cls=TextLoader,\n","#        show_progress=True,\n","#    )\n","#    docs = loader.load()\n","#    print(f\"Loaded {len(docs)} document(s) from {data_path}\")\n","#    return docs"],"metadata":{"id":"86RBO_RXMzsy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# 1. INDEXING: load → split → store\n","# For .txt, .md, .pdf.\n","# -------------------------\n","\n","from pathlib import Path\n","from langchain_community.document_loaders import TextLoader, PyPDFLoader\n","\n","\n","def load_local_documents(data_path: str = \"./data\"):\n","    \"\"\"\n","    Loads .txt, .md and .pdf files from `data/`.\n","    Automatically chooses the right loader per extension.\n","    \"\"\"\n","    folder = Path(data_path)\n","    if not folder.exists():\n","        raise RuntimeError(f\"Folder not found: {data_path}\")\n","\n","    doc_paths = list(folder.rglob(\"*.*\"))\n","    if not doc_paths:\n","        raise RuntimeError(f\"No documents found in {data_path}\")\n","\n","    loaded_docs = []\n","\n","    for path in doc_paths:\n","        print(f\"Processing {path}\")\n","        ext = path.suffix.lower()\n","\n","        try:\n","            if ext in [\".txt\", \".md\", \".rst\", \".log\"]:\n","                # Treat markdown & text alike\n","                loader = TextLoader(str(path), encoding=\"utf-8\")\n","                docs = loader.load()\n","\n","            elif ext == \".pdf\":\n","                loader = PyPDFLoader(str(path))\n","                docs = loader.load()\n","\n","            else:\n","                print(f\"Skipping unsupported file: {path.name}\")\n","                continue\n","\n","            # annotate metadata with filename\n","            for d in docs:\n","                d.metadata[\"source_file\"] = path.name\n","\n","            loaded_docs.extend(docs)\n","            print(f\"Loaded {path.name} ({len(docs)} pages/chunks)\")\n","\n","        except Exception as e:\n","            print(f\"Error loading {path.name}: {e}\")\n","\n","    print(f\"\\nTotal loaded documents: {len(loaded_docs)}\")\n","    return loaded_docs\n","\n","\n","def build_vector_store(docs):\n","    \"\"\"\n","    Split documents into chunks and index them in an in-memory vector store.\n","    \"\"\"\n","    # 1) Split into chunks (RAG-friendly)\n","    splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=1000,\n","        chunk_overlap=200,\n","        add_start_index=True,\n","    )\n","    splits = splitter.split_documents(docs)\n","    print(f\"Split into {len(splits)} chunk(s)\")\n","\n","    # 2) Embeddings + vector store\n","    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n","    vector_store = InMemoryVectorStore(embeddings)\n","    vector_store.add_documents(splits)\n","    print(\"Indexed chunks in InMemoryVectorStore\")\n","\n","    return vector_store\n"],"metadata":{"id":"IURaBbHXH96U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# 2. TOOL: retrieval function for RAG\n","# -------------------------\n","\n","from langchain.tools import tool\n","\n","def build_retrieve_tool(vector_store):\n","    \"\"\"\n","    Wraps the vector store in a LangChain tool.\n","    The tool returns context annotated with file name and page number so\n","    the agent can mention them in its answer.\n","    \"\"\"\n","\n","    @tool(response_format=\"content_and_artifact\")\n","    def retrieve_context(query: str):\n","        \"\"\"\n","        Retrieve information from your local knowledge base to help answer a query.\n","\n","        The returned text is annotated like:\n","        [DOC 1] file=policy_terms.txt page=1\n","        ...\n","        \"\"\"\n","        retrieved_docs = vector_store.similarity_search(query, k=4)\n","\n","        blocks = []\n","        for i, doc in enumerate(retrieved_docs, start=1):\n","            source_file = (\n","                doc.metadata.get(\"source_file\")\n","                or doc.metadata.get(\"source\")\n","                or \"unknown_file\"\n","            )\n","            page = doc.metadata.get(\"page\")  # PyPDFLoader usually sets this\n","            if page is None:\n","                page_info = \"\"\n","            else:\n","                page_info = f\", page={page + 1}\" if isinstance(page, int) else f\", page={page}\"\n","\n","            header = f\"[DOC {i}] file={source_file}{page_info}\"\n","            blocks.append(f\"{header}\\n{doc.page_content.strip()}\")\n","\n","        serialized = \"\\n\\n\".join(blocks)\n","\n","        # 1) `serialized` -> text the model sees as tool output\n","        # 2) `retrieved_docs` -> kept as artifacts for richer tooling if needed\n","        return serialized, retrieved_docs\n","\n","    return retrieve_context\n"],"metadata":{"id":"4BEU9IPQNLGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# 3. AGENT: model + tools using create_agent\n","# -------------------------\n","\n","from langchain.chat_models import init_chat_model\n","from langchain.agents import create_agent\n","\n","def build_rag_agent(retrieve_tool):\n","    \"\"\"\n","    Build a graph-based agent that can call the retrieve_context tool.\n","    It will also mention which document(s) each answer comes from.\n","    \"\"\"\n","    model = init_chat_model(\"gpt-4.1-mini\")\n","    tools = [retrieve_tool]\n","\n","    system_prompt = \"\"\"\n","You are a helpful assistant over a local knowledge base of documents.\n","\n","- If the user asks anything that could depend on the local files,\n","  you MUST first call the `retrieve_context` tool.\n","- The tool returns context annotated with labels like:\n","  [DOC 1] file=claims_process.md, page=2\n","  [DOC 2] file=risk_assessment_guide.pdf, page=1\n","\n","When you answer:\n","- Use these DOC labels to show where information comes from.\n","- Explicitly mention the document name (and page if available) in your answer.\n","  Example: \"The SLA for claim review is 15 working days (source: DOC 1 - claims_process.md).\"\n","- If you use multiple documents, you can cite them like:\n","  \"(sources: DOC 1 - claims_process.md; DOC 3 - risk_assessment_guide.pdf, page 2)\"\n","- If the question is clearly general knowledge and not about the files,\n","  you may answer directly, and say you are not using the local documents.\n","\n","Always answer clearly and concisely.\n","\"\"\".strip()\n","\n","    agent = create_agent(\n","        model=model,\n","        tools=tools,\n","        system_prompt=system_prompt,\n","    )\n","    return agent"],"metadata":{"id":"KcKq0jBpNUGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# 4. SIMPLE CLI LOOP\n","# -------------------------\n","\n","\n","def run_cli(agent):\n","    \"\"\"\n","    Very simple REPL that sends each user message as a new 'thread'.\n","    Uses agent.stream(...) to let the agent call tools and think in steps. :contentReference[oaicite:9]{index=9}\n","    \"\"\"\n","    print(\"\\n--- RAG Agent ready! Ask questions about your documents. ---\")\n","    print(\"Type 'exit' or 'quit' to stop.\\n\")\n","\n","    while True:\n","        user_input = input(\"You: \").strip()\n","        if not user_input:\n","            continue\n","        if user_input.lower() in {\"exit\", \"quit\"}:\n","            print(\"Bye!\")\n","            break\n","\n","        # We send the conversation as a list of messages;\n","        # here we just send the latest user message.\n","        events = agent.stream(\n","            {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n","            stream_mode=\"values\",\n","        )\n","\n","        last_state = None\n","        for state in events:\n","            last_state = state  # we only care about the final state\n","\n","        if not last_state:\n","            print(\"Agent: (no response)\")\n","            continue\n","\n","        # The state is a dict with a `messages` list; last one is the final AI message.\n","        final_msg = last_state[\"messages\"][-1]\n","        content = final_msg.content\n","\n","        if isinstance(content, list):\n","            # Sometimes content is a list of parts; join any text parts.\n","            text = \"\".join(\n","                part.get(\"text\", \"\") if isinstance(part, dict) else str(part)\n","                for part in content\n","            )\n","        else:\n","            text = content\n","\n","        print(f\"\\nAgent: {text}\\n\")"],"metadata":{"id":"fiGgmf13NXNK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# 5. MAIN\n","# -------------------------\n","\n","\n","def main():\n","    if \"OPENAI_API_KEY\" not in os.environ:\n","        raise RuntimeError(\"Please set the OPENAI_API_KEY environment variable.\")\n","\n","    docs = load_local_documents(\"./data\")\n","    if not docs:\n","        raise RuntimeError(\n","            \"No .txt files found in ./data. Add some documents before running.\"\n","        )\n","\n","    vector_store = build_vector_store(docs)\n","    retrieve_tool = build_retrieve_tool(vector_store)\n","    agent = build_rag_agent(retrieve_tool)\n","\n","    run_cli(agent)"],"metadata":{"id":"WvRDaFj6NZFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"VFYLe3iDNbTR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Try questions like:\n","\n","“Summarize the key insurance terms from the documents.”\n","\n","“What SLA penalties are mentioned?”\n","\n","“What risk evaluation methods does the policy mandate?”\n","\n","“Give me a combined summary of all documents.”"],"metadata":{"id":"C9uGPr_RMX-C"}},{"cell_type":"code","source":[],"metadata":{"id":"uJu0UAzsKp3X"},"execution_count":null,"outputs":[]}]}