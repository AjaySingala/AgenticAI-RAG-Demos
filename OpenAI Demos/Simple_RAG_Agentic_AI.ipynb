{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MskO1W1RcLO-"
   },
   "source": [
    "### Setup local Python environment.\n",
    "cd path/to/your/folder\n",
    "\n",
    "python -m venv venv\n",
    "\n",
    "venv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YfFGRue3MJS"
   },
   "source": [
    "# Minimal RAG + Agentic AI example using Python + OpenAI.\n",
    "\n",
    "What it shows:\n",
    "- Building a tiny in-memory vector store with OpenAI embeddings\n",
    "- An \"agent\" (LLM) that decides when to call retrieval\n",
    "- A simple agent loop: ask -> maybe retrieve -> answer\n",
    "\n",
    "Prereqs:\n",
    "\n",
    "    pip install openai numpy\n",
    "\n",
    "Set env:\n",
    "\n",
    "    export OPENAI_API_KEY=\"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnrAa7pg2Ym9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmlU85BB4Bxr"
   },
   "outputs": [],
   "source": [
    "# OpenAI API Key.\n",
    "\n",
    "# For Google Colab environment.\n",
    "from google.colab import userdata\n",
    "key = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# For local environment.\n",
    "#import os\n",
    "#\n",
    "#key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not key:\n",
    "    raise ValueError(\"API key not found. Please set the MY_API_KEY environment variable.\")\n",
    "\n",
    "print(\"API Key loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1uCSBuR3Uqt"
   },
   "outputs": [],
   "source": [
    "# ---------- OpenAI client ----------\n",
    "#client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "client = OpenAI(api_key=key)\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"   # small, cheap embeddings\n",
    "CHAT_MODEL = \"gpt-4.1-mini\"                 # fast, good for tools/agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDPJu_9B3afW"
   },
   "outputs": [],
   "source": [
    "# ---------- 1. Tiny knowledge base (you'd replace this with real docs) ----------\n",
    "\n",
    "KB_DOCS = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"title\": \"Claims submission process\",\n",
    "        \"text\": (\n",
    "            \"Customers must submit claims within 30 days of the incident. \"\n",
    "            \"They should provide policy number, date of incident, and all supporting documents. \"\n",
    "            \"Claims can be submitted via the mobile app or the web portal.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"title\": \"Fraud detection policy\",\n",
    "        \"text\": (\n",
    "            \"Suspicious claims are flagged when claim amount is unusually high compared to \"\n",
    "            \"customer's historical patterns or when multiple claims are filed in a short time. \"\n",
    "            \"Flagged claims go to the special investigations unit.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"title\": \"Refund and cancellation rules\",\n",
    "        \"text\": (\n",
    "            \"Policyholders can cancel within the first 15 days for a full refund, \"\n",
    "            \"provided no claims have been filed. After that, pro-rated refunds apply.\"\n",
    "        ),\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7epHw95k44Rr"
   },
   "outputs": [],
   "source": [
    "# ---------- 2. Build simple in-memory vector store ----------\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Call OpenAI embeddings API.\"\"\"\n",
    "    resp = client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=text,\n",
    "    )\n",
    "    return resp.data[0].embedding\n",
    "\n",
    "print(\"Building vector store...\")\n",
    "KB_EMBEDDINGS = np.array([get_embedding(doc[\"text\"]) for doc in KB_DOCS])\n",
    "KB_IDS = [doc[\"id\"] for doc in KB_DOCS]\n",
    "print(\"Vector store ready with\", len(KB_DOCS), \"documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhuYAtNb4-TD"
   },
   "outputs": [],
   "source": [
    "# Function to Search.\n",
    "def search_knowledge_base(query: str, k: int = 2) -> List[Dict]:\n",
    "    \"\"\"Simple cosine-similarity search over KB_DOCS.\n",
    "    Cosine-similarity search is a method used to find how similar two pieces of text \n",
    "    (or any high-dimensional vectors) are, based on the angle between them.\n",
    "    \"\"\"\n",
    "    q_emb = np.array(get_embedding(query))\n",
    "\n",
    "    doc_norms = np.linalg.norm(KB_EMBEDDINGS, axis=1)\n",
    "    q_norm = np.linalg.norm(q_emb)\n",
    "    sims = KB_EMBEDDINGS @ q_emb / (doc_norms * q_norm + 1e-8)\n",
    "\n",
    "    top_idx = sims.argsort()[-k:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        doc = KB_DOCS[i]\n",
    "        results.append(\n",
    "            {\n",
    "                \"id\": doc[\"id\"],\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"score\": float(sims[i]),\n",
    "                \"text\": doc[\"text\"],\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YZXPc3765zL"
   },
   "outputs": [],
   "source": [
    "# Format the result.\n",
    "def format_retrieval_results(results: List[Dict]) -> str:\n",
    "    \"\"\"Turn retrieved docs into a context block for the model.\"\"\"\n",
    "    lines = []\n",
    "    for r in results:\n",
    "        lines.append(f\"[{r['id']}] {r['title']} (score={r['score']:.3f})\")\n",
    "        lines.append(r[\"text\"])\n",
    "        lines.append(\"\")  # blank line\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ydug16-k65ry"
   },
   "outputs": [],
   "source": [
    "# ---------- 3. Agent prompt ----------\n",
    "\n",
    "AGENT_SYSTEM_PROMPT = \"\"\"\n",
    "You are an internal knowledge RAG agent for an insurance company.\n",
    "\n",
    "You have two modes:\n",
    "\n",
    "1) If you need company knowledge to answer:\n",
    "   - Respond *only* with a single line:\n",
    "     CALL_RETRIEVER: {\"query\": \"<short search query>\"}\n",
    "\n",
    "2) If you already have enough information (including any retrieved context):\n",
    "   - Answer the user clearly.\n",
    "   - Cite document IDs like [doc1], [doc2] when using internal knowledge.\n",
    "\n",
    "Never invent the CALL_RETRIEVER line unless you really need more context.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njt6pPar65kF"
   },
   "outputs": [],
   "source": [
    "# ---------- 4. Agent loop (Agentic RAG) ----------\n",
    "\n",
    "def run_rag_agent(user_question: str, max_tool_loops: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Agentic RAG loop:\n",
    "        user question -> model may call retriever -> we search + add context -> model answers.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": AGENT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_question},\n",
    "    ]\n",
    "\n",
    "    for step in range(max_tool_loops + 1):\n",
    "        # 1) Ask the model what to do next\n",
    "        chat_resp = client.chat.completions.create(\n",
    "            model=CHAT_MODEL,\n",
    "            messages=messages,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        reply = chat_resp.choices[0].message.content.strip()\n",
    "        print(f\"\\n--- Agent step {step} ---\")\n",
    "        print(\"MODEL RAW REPLY:\\n\", reply)\n",
    "\n",
    "        # 2) If the model calls the retriever, execute tool + loop again\n",
    "        if reply.startswith(\"CALL_RETRIEVER:\"):\n",
    "            try:\n",
    "                json_str = reply.split(\"CALL_RETRIEVER:\", 1)[1].strip()\n",
    "                tool_args = json.loads(json_str)\n",
    "                search_query = tool_args[\"query\"]\n",
    "            except Exception as e:\n",
    "                # Fallback: treat as normal answer if parsing failed\n",
    "                print(\"Failed to parse retriever call:\", e)\n",
    "                return reply\n",
    "\n",
    "            # Run our retrieval tool\n",
    "            results = search_knowledge_base(search_query, k=2)\n",
    "            context_block = format_retrieval_results(results)\n",
    "\n",
    "            # Add this interaction to the conversation\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": reply,  # the tool call itself\n",
    "            })\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"Retrieved internal knowledge:\\n{context_block}\",\n",
    "            })\n",
    "\n",
    "            # Continue loop; the agent will now answer using this context\n",
    "            continue\n",
    "\n",
    "        # 3) If there's no retriever call, we treat this as the final answer\n",
    "        return reply\n",
    "\n",
    "    return \"Sorry, I couldn't complete the reasoning in the allowed steps.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ylb-kUNc65cD"
   },
   "outputs": [],
   "source": [
    "# ---------- 5. Small demo ----------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question = (\n",
    "        \"A customer filed a claim 40 days after the incident and wants a refund \"\n",
    "        \"for cancelling the policy. What should we tell them? Use internal rules.\"\n",
    "    )\n",
    "    final_answer = run_rag_agent(question)\n",
    "    print(\"\\n=== FINAL ANSWER ===\")\n",
    "    print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM8YrZiuE/1VCtl6XvAnINB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
