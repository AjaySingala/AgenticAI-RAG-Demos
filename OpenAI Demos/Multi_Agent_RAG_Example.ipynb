{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGMfdp8+k8GHF/LIVi8h7E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Setup local Python environment.\n","cd path/to/your/folder\n","\n","python -m venv venv\n","\n","venv\\Scripts\\activate"],"metadata":{"id":"8SXVReqRcEMe"}},{"cell_type":"markdown","source":["# Agents:\n","  1. Planner Agent  – understands the user question and decides what to retrieve.\n","  2. Retrieval Agent – implemented in Python using OpenAI embeddings (vector search).\n","  3. Writer Agent   – writes the final answer using the retrieved context.\n","\n","Prereqs:\n","\n","    pip install openai numpy\n","\n","Env:\n","\n","    export OPENAI_API_KEY=\"sk-...\""],"metadata":{"id":"23qT_XNI7xTi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9l9FZ5w7h0Q"},"outputs":[],"source":["import os\n","import json\n","from typing import List, Dict\n","import numpy as np\n","from openai import OpenAI\n"]},{"cell_type":"code","source":["# OpenAI API Key.\n","\n","# For Google Colab environment.\n","from google.colab import userdata\n","key = userdata.get('OPENAI_API_KEY')\n","\n","# For local environment.\n","#import os\n","#\n","#key = os.getenv(\"OPENAI_API_KEY\")\n","\n","if not key:\n","    raise ValueError(\"API key not found. Please set the MY_API_KEY environment variable.\")\n","\n","print(\"API Key loaded successfully!\")"],"metadata":{"id":"ca4zVHdY74Ro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- OpenAI setup --------------------\n","#client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n","client = OpenAI(api_key=key)\n","\n","EMBEDDING_MODEL = \"text-embedding-3-small\"\n","CHAT_MODEL = \"gpt-4.1-mini\""],"metadata":{"id":"1o3uOVB474LW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- 0. Tiny knowledge base --------------------\n","\n","KB_DOCS = [\n","    {\n","        \"id\": \"doc1\",\n","        \"title\": \"Claims submission process\",\n","        \"text\": (\n","            \"Customers must submit claims within 30 days of the incident. \"\n","            \"They should provide policy number, date of incident, and all supporting documents. \"\n","            \"Claims can be submitted via the mobile app or the web portal.\"\n","        ),\n","    },\n","    {\n","        \"id\": \"doc2\",\n","        \"title\": \"Fraud detection policy\",\n","        \"text\": (\n","            \"Suspicious claims are flagged when claim amount is unusually high compared to \"\n","            \"customer's historical patterns or when multiple claims are filed in a short time. \"\n","            \"Flagged claims go to the special investigations unit.\"\n","        ),\n","    },\n","    {\n","        \"id\": \"doc3\",\n","        \"title\": \"Refund and cancellation rules\",\n","        \"text\": (\n","            \"Policyholders can cancel within the first 15 days for a full refund, \"\n","            \"provided no claims have been filed. After that, pro-rated refunds apply.\"\n","        ),\n","    },\n","]"],"metadata":{"id":"GPz4od4u74F3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- 1. Vector store utilities (Retrieval Agent core) --------------------\n","\n","def get_embedding(text: str) -> List[float]:\n","    \"\"\"Call OpenAI embeddings API.\"\"\"\n","    resp = client.embeddings.create(\n","        model=EMBEDDING_MODEL,\n","        input=text,\n","    )\n","    return resp.data[0].embedding\n","\n","print(\"Building vector store for KB...\")\n","KB_EMBEDDINGS = np.array([get_embedding(doc[\"text\"]) for doc in KB_DOCS])\n","KB_IDS = [doc[\"id\"] for doc in KB_DOCS]\n","print(f\"Vector store ready with {len(KB_DOCS)} documents.\\n\")"],"metadata":{"id":"W6iPQ-vt74Aa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def search_knowledge_base(query: str, k: int = 2) -> List[Dict]:\n","    \"\"\"Simple cosine similarity search.\"\"\"\n","    q_emb = np.array(get_embedding(query))\n","\n","    doc_norms = np.linalg.norm(KB_EMBEDDINGS, axis=1)\n","    q_norm = np.linalg.norm(q_emb)\n","    sims = KB_EMBEDDINGS @ q_emb / (doc_norms * q_norm + 1e-8)\n","\n","    top_idx = sims.argsort()[-k:][::-1]\n","\n","    results = []\n","    for i in top_idx:\n","        doc = KB_DOCS[i]\n","        results.append(\n","            {\n","                \"id\": doc[\"id\"],\n","                \"title\": doc[\"title\"],\n","                \"score\": float(sims[i]),\n","                \"text\": doc[\"text\"],\n","            }\n","        )\n","    return results"],"metadata":{"id":"rqAo4Mll736_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def format_retrieval_results(results: List[Dict]) -> str:\n","    \"\"\"Turn retrieved docs into a context block for the Writer Agent.\"\"\"\n","    lines = []\n","    for r in results:\n","        lines.append(f\"[{r['id']}] {r['title']} (score={r['score']:.3f})\")\n","        lines.append(r[\"text\"])\n","        lines.append(\"\")  # blank line\n","    return \"\\n\".join(lines)"],"metadata":{"id":"vX3zKir9731v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- 2. Planner Agent --------------------\n","\n","PLANNER_SYSTEM_PROMPT = \"\"\"\n","You are the Planner Agent in a multi-agent RAG system for an insurance company.\n","\n","Your job:\n","- Understand the user's question.\n","- Decide what information is needed from the internal knowledge base.\n","- Produce:\n","    - one or more short retrieval queries, and\n","    - a short plan for how the final answer should be structured.\n","\n","You MUST respond ONLY in the following JSON format (no extra text):\n","\n","{\n","  \"retrieval_queries\": [\"...\"],\n","  \"answer_plan\": \"...\"\n","}\n","\"\"\"\n","\n","def call_planner_agent(user_question: str) -> Dict:\n","    messages = [\n","        {\"role\": \"system\", \"content\": PLANNER_SYSTEM_PROMPT},\n","        {\"role\": \"user\", \"content\": user_question},\n","    ]\n","    resp = client.chat.completions.create(\n","        model=CHAT_MODEL,\n","        messages=messages,\n","        temperature=0.2,\n","    )\n","\n","    content = resp.choices[0].message.content.strip()\n","    print(\"=== PLANNER AGENT OUTPUT ===\")\n","    print(content)\n","    print(\"=============================\\n\")\n","\n","    try:\n","        data = json.loads(content)\n","    except json.JSONDecodeError:\n","        # Fall back to a safe default if parsing fails\n","        data = {\n","            \"retrieval_queries\": [user_question],\n","            \"answer_plan\": \"Explain the situation and refer to relevant internal rules.\"\n","        }\n","    return data"],"metadata":{"id":"ovG6fMkd73ww"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- 3. Retrieval Agent (Python tool) --------------------\n","\n","def call_retrieval_agent(retrieval_queries: List[str], k_per_query: int = 2) -> List[Dict]:\n","    \"\"\"\n","    The 'Retrieval Agent' is implemented as Python logic over embeddings.\n","    It:\n","      - takes planner's retrieval_queries\n","      - performs semantic search for each\n","      - merges and de-duplicates results\n","    \"\"\"\n","    all_results: Dict[str, Dict] = {}\n","\n","    for q in retrieval_queries:\n","        print(f\"Retrieval Agent: searching for query -> {q!r}\")\n","        results = search_knowledge_base(q, k=k_per_query)\n","        for r in results:\n","            doc_id = r[\"id\"]\n","            # Keep the best score per document\n","            if doc_id not in all_results or r[\"score\"] > all_results[doc_id][\"score\"]:\n","                all_results[doc_id] = r\n","\n","    merged_results = list(all_results.values())\n","    print(\"\\n=== RETRIEVAL AGENT MERGED RESULTS ===\")\n","    for r in merged_results:\n","        print(f\"- {r['id']} | {r['title']} | score={r['score']:.3f}\")\n","    print(\"======================================\\n\")\n","\n","    return merged_results"],"metadata":{"id":"CIM9iHWC73r2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- 4. Writer Agent --------------------\n","\n","WRITER_SYSTEM_PROMPT = \"\"\"\n","You are the Writer Agent in a multi-agent RAG system for an insurance company.\n","\n","You receive:\n","- The original user question.\n","- An answer plan created by the Planner Agent.\n","- Retrieved internal knowledge snippets with IDs like [doc1], [doc2].\n","\n","Your job:\n","- Follow the answer plan.\n","- Use the retrieved knowledge as the primary source of truth.\n","- Clearly explain the reasoning.\n","- Cite document IDs like [doc1] where relevant.\n","- If something is not covered by the documents, say so explicitly.\n","\n","Be concise and professional.\n","\"\"\"\n","\n","def call_writer_agent(user_question: str, answer_plan: str, retrieved_context: str) -> str:\n","    messages = [\n","        {\"role\": \"system\", \"content\": WRITER_SYSTEM_PROMPT},\n","        {\n","            \"role\": \"user\",\n","            \"content\": (\n","                f\"User question:\\n{user_question}\\n\\n\"\n","                f\"Answer plan from Planner Agent:\\n{answer_plan}\\n\\n\"\n","                f\"Retrieved internal knowledge:\\n{retrieved_context}\"\n","            ),\n","        },\n","    ]\n","    resp = client.chat.completions.create(\n","        model=CHAT_MODEL,\n","        messages=messages,\n","        temperature=0.2,\n","    )\n","    final_answer = resp.choices[0].message.content.strip()\n","\n","    print(\"=== WRITER AGENT OUTPUT ===\")\n","    print(final_answer)\n","    print(\"===========================\\n\")\n","\n","    return final_answer"],"metadata":{"id":"B7rGONrW73mG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- 5. Orchestrator: Multi-Agent RAG Pipeline --------------------\n","\n","def run_multi_agent_rag(user_question: str) -> str:\n","    \"\"\"\n","    High-level orchestration:\n","      1. Planner Agent -> retrieval_queries + answer_plan\n","      2. Retrieval Agent -> relevant docs\n","      3. Writer Agent -> final answer\n","    \"\"\"\n","    # 1) Planner\n","    planner_output = call_planner_agent(user_question)\n","    retrieval_queries = planner_output.get(\"retrieval_queries\", [user_question])\n","    answer_plan = planner_output.get(\"answer_plan\", \"Explain answer step by step.\")\n","\n","    # 2) Retrieval\n","    retrieved_docs = call_retrieval_agent(retrieval_queries, k_per_query=2)\n","    context_block = format_retrieval_results(retrieved_docs)\n","\n","    # 3) Writer\n","    final_answer = call_writer_agent(user_question, answer_plan, context_block)\n","    return final_answer"],"metadata":{"id":"6ugpSAmS8Sa1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- 6. Demo --------------------\n","\n","if __name__ == \"__main__\":\n","    question = (\n","        \"A customer filed a claim 40 days after the incident and now wants to cancel the \"\n","        \"policy and receive a full refund. Based on our internal rules, what should we tell them?\"\n","    )\n","    answer = run_multi_agent_rag(question)\n","    print(\"\\n=== FINAL ANSWER (RETURNED TO USER) ===\")\n","    print(answer)"],"metadata":{"id":"xrSgVnVc8SXY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"U17IQffN8nKL"},"execution_count":null,"outputs":[]}]}