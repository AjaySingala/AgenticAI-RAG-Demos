{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j6jie8P9ZNZ"
   },
   "source": [
    "# Minimal RAG Agentic AI demo with AutoGen (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eEKogA4b4fi"
   },
   "source": [
    "### Setup local Python environment.\n",
    "cd path/to/your/folder\n",
    "\n",
    "python -m venv venv\n",
    "\n",
    "venv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsJfSqgZ9qCv"
   },
   "outputs": [],
   "source": [
    "%pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\" chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ocn2CHBDGono"
   },
   "outputs": [],
   "source": [
    "#!pip show autogen-agentchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fcz9QqGG9aLv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# OpenAI API Key.\n",
    "\n",
    "# For Google Colab environment.\n",
    "#from google.colab import userdata\n",
    "#key = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# For local environment.\n",
    "import os\n",
    "\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not key:\n",
    "    raise ValueError(\"API key not found. Please set the MY_API_KEY environment variable.\")\n",
    "\n",
    "print(\"API Key loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiilvVffGawI"
   },
   "source": [
    "### Simple RAG index with Chroma\n",
    "\n",
    "This is a tiny, self-contained RAG backend: load files from ./data, chunk text, store in Chroma, and expose a rag_search() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yM8i2gGN9Z61"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import textwrap\n",
    "from typing import List\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QELF_T_s9Z35"
   },
   "outputs": [],
   "source": [
    "# ---------- 1) Create a Chroma client & embedding function ----------\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    #api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_key=key,\n",
    "    model_name=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "COLLECTION_NAME = \"demo_docs\"\n",
    "collection = None  # will be created inside index_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hDL_0Ejh9Z1P"
   },
   "outputs": [],
   "source": [
    "# ---------- 2) Helper: read all text-like files ----------\n",
    "def read_file(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def get_all_files(data_dir: str = \"./data\") -> List[str]:\n",
    "    exts = (\"*.txt\", \"*.md\")\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files.extend(glob.glob(os.path.join(data_dir, ext)))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mQqRDwrG9Zx6"
   },
   "outputs": [],
   "source": [
    "# ---------- 3) SAFE chunking implementation ----------\n",
    "def chunk_text(text: str, max_chars: int = 800, overlap: int = 120) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks, without infinite loops.\n",
    "    - If text shorter than max_chars: return single chunk.\n",
    "    - Otherwise: advance forward; for the last chunk, we stop when end == n.\n",
    "    \"\"\"\n",
    "    n = len(text)\n",
    "    if n <= max_chars:\n",
    "        return [text]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < n:\n",
    "        end = min(start + max_chars, n)\n",
    "        chunks.append(text[start:end])\n",
    "\n",
    "        if end == n:\n",
    "            break  # reached the end safely\n",
    "\n",
    "        # move forward with overlap\n",
    "        start = max(0, end - overlap)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8gdId7-9Zu8"
   },
   "outputs": [],
   "source": [
    "# ---------- 4) Index docs into a fresh collection ----------\n",
    "def index_docs(data_dir: str = \"./data\"):\n",
    "    global collection\n",
    "\n",
    "    files = get_all_files(data_dir)\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No .txt/.md files found in {data_dir}\")\n",
    "\n",
    "    # Always recreate the collection for this demo\n",
    "    try:\n",
    "        chroma_client.delete_collection(COLLECTION_NAME)\n",
    "    except Exception:\n",
    "        pass  # it's fine if it didn't exist yet\n",
    "\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=embedding_fn,\n",
    "    )\n",
    "\n",
    "    doc_ids = []\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for path in files:\n",
    "        print(f\"Processing {path}\")\n",
    "        raw = read_file(path)\n",
    "        chunks = chunk_text(raw, max_chars=800, overlap=120)\n",
    "\n",
    "        for i, ch in enumerate(chunks):\n",
    "            doc_ids.append(f\"{os.path.basename(path)}__{i}\")\n",
    "            texts.append(ch)\n",
    "            metadatas.append({\"source\": os.path.basename(path), \"chunk\": i})\n",
    "\n",
    "    collection.add(ids=doc_ids, documents=texts, metadatas=metadatas)\n",
    "    print(f\"Indexed {len(texts)} chunks from {len(files)} files.\")\n",
    "\n",
    "\n",
    "# Call this once after your docs are in ./data\n",
    "index_docs(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIzNzyYtIgDl"
   },
   "source": [
    "### Define a RAG tool (rag_search) for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dfqkMt39Zrt"
   },
   "outputs": [],
   "source": [
    "async def rag_search(query: str, top_k: int = 4) -> str:\n",
    "    \"\"\"\n",
    "    Tool: search internal docs for relevant snippets.\n",
    "    Returns a concatenated context string the model can use.\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k,\n",
    "    )\n",
    "\n",
    "    docs = results.get(\"documents\", [[]])[0]\n",
    "    metas = results.get(\"metadatas\", [[]])[0]\n",
    "\n",
    "    if not docs:\n",
    "        return \"No relevant context found in the knowledge base.\"\n",
    "\n",
    "    formatted_chunks = []\n",
    "    for doc, meta in zip(docs, metas):\n",
    "        # source identifies where the document chunk came from (e.g., filename, URL).\n",
    "        # chunk identifies the chunk number or segment ID.\n",
    "        # If the metadata doesn't contain those keys, defaults are used:\n",
    "        #   - \"unknown\" for source\n",
    "        #   - 0 for chunk_id\n",
    "        src = meta.get(\"source\", \"unknown\")\n",
    "        chunk_id = meta.get(\"chunk\", 0)\n",
    "\n",
    "        # Removes newlines from doc,\n",
    "        # Shortens it to a maximum width of 400 characters,\n",
    "        # If shortened, adds \" ...\" at the end\n",
    "        snippet = textwrap.shorten(doc.replace(\"\\n\", \" \"), width=400, placeholder=\" ...\")\n",
    "        formatted_chunks.append(f\"[{src}#{chunk_id}] {snippet}\")\n",
    "\n",
    "    return \"\\n\".join(formatted_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pla1MaV0IwEs"
   },
   "source": [
    "### Create the RAG assistant (new autogen_agentchat style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wSBLIXzXIh6a"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yh4AGCbMIh3f"
   },
   "outputs": [],
   "source": [
    "# ---------- 1) Model client ----------\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4.1-mini\",   # or gpt-4o, gpt-4.1, etc.\n",
    "    api_key=key,\n",
    ")\n",
    "\n",
    "# ---------- 2) RAG Answering Agent ----------\n",
    "rag_agent = AssistantAgent(\n",
    "    name=\"rag_assistant\",\n",
    "    model_client=model_client,\n",
    "    system_message=(\n",
    "        \"You are a Q&A assistant over our internal security documentation.\\n\"\n",
    "        \"- Use the `rag_search` tool to fetch relevant context.\\n\"\n",
    "        \"- Ground your answers ONLY in that context.\\n\"\n",
    "        \"- If the answer is not in context, say you don't know.\\n\"\n",
    "    ),\n",
    "    tools=[rag_search],          # <-- our RAG tool\n",
    "    max_tool_iterations=2,       # let it call the tool at most twice\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7uE7H3sJIbV"
   },
   "source": [
    "### Helper to pull out the final answer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iusvZSnPIh0U"
   },
   "outputs": [],
   "source": [
    "from autogen_agentchat.base import TaskResult\n",
    "\n",
    "def extract_last_text(result: TaskResult) -> str:\n",
    "    # Find the last TextMessage in the task result\n",
    "    for msg in reversed(result.messages):\n",
    "        if isinstance(msg, TextMessage):\n",
    "            return msg.content\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUioKggxJNDH"
   },
   "source": [
    "### Optional second agent: Writer / Refiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "q7Hy6F77Ihxk"
   },
   "outputs": [],
   "source": [
    "writer_agent = AssistantAgent(\n",
    "    name=\"writer_agent\",\n",
    "    model_client=model_client,\n",
    "    system_message=(\n",
    "        \"You are a senior technical writer.\\n\"\n",
    "        \"Rewrite answers for the specified audience in clear, simple language.\\n\"\n",
    "        \"Keep it concise and structured with bullets where useful.\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCPMDh9sJR8e"
   },
   "source": [
    "### Refinement helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WldjT-G9Ihuc"
   },
   "outputs": [],
   "source": [
    "async def refine_for_audience(raw_answer: str,\n",
    "                              audience: str = \"non-technical business stakeholder\",\n",
    "                              max_words: int = 200) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Here is an AI-generated answer:\n",
    "\n",
    "    ---\n",
    "    {raw_answer}\n",
    "    ---\n",
    "\n",
    "    TASK:\n",
    "    1. Rewrite this for the audience: {audience}.\n",
    "    2. Use clear, plain language and avoid heavy jargon.\n",
    "    3. Keep it under {max_words} words.\n",
    "    4. Use bullets or short paragraphs for readability.\n",
    "    \"\"\"\n",
    "\n",
    "    result = await writer_agent.run(task=textwrap.dedent(prompt).strip())\n",
    "    return extract_last_text(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmxxrcMiJb5d"
   },
   "source": [
    "### Full end-to-end Agentic RAG flow\n",
    "\n",
    "This is your agentic pipeline without RetrieveUserProxyAgent:\n",
    "\n",
    "1. rag_agent uses a RAG tool (rag_search) to get context from Chroma.\n",
    "\n",
    "2. It generates a grounded answer.\n",
    "\n",
    "3. writer_agent rewrites the answer for a particular audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CR7evlwcIhrK"
   },
   "outputs": [],
   "source": [
    "async def rag_then_refine(question: str):\n",
    "    print(\"\\n\" + \"#\" * 80)\n",
    "    print(\"QUESTION:\", question)\n",
    "    print(\"#\" * 80)\n",
    "\n",
    "    # 1) Ask the RAG assistant\n",
    "    task = (\n",
    "        \"Answer the following question using ONLY the internal docs. \"\n",
    "        \"ALWAYS call the `rag_search` tool first to fetch context.\\n\\n\"\n",
    "        f\"Question: {question}\"\n",
    "    )\n",
    "    rag_result = await rag_agent.run(task=task)\n",
    "    raw_answer = extract_last_text(rag_result)\n",
    "\n",
    "    print(\"\\n--- RAW RAG ANSWER ---\\n\")\n",
    "    print(raw_answer)\n",
    "\n",
    "    # 2) Refine for executives\n",
    "    refined = await refine_for_audience(\n",
    "        raw_answer,\n",
    "        audience=\"C-level executive with limited technical background\",\n",
    "        max_words=180,\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- REFINED FOR EXECUTIVES ---\\n\")\n",
    "    print(refined)\n",
    "    print(\"\\n\" + \"#\" * 80 + \"\\n\")\n",
    "\n",
    "    return raw_answer, refined\n",
    "\n",
    "\n",
    "# Run one or two demo questions\n",
    "async def main():\n",
    "    await rag_then_refine(\"What are the main steps in our security review process?\")\n",
    "    await rag_then_refine(\"Summarize the key benefits of our security policy.\")\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqsBd9xWIhn1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMYRQ6vGIXBr0o8Jwv+wSJG",
   "mount_file_id": "1sfl-3STXff51CGHVBeLW48ms3CWQfV4g",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
