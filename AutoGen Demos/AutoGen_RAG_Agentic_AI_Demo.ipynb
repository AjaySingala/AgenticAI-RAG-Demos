{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPkAwVeVQDWq2L7liZsPrbm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Minimal RAG Agentic AI demo with AutoGen (Python)"],"metadata":{"id":"5j6jie8P9ZNZ"}},{"cell_type":"markdown","source":["### Setup local Python environment.\n","cd path/to/your/folder\n","\n","python -m venv venv\n","\n","venv\\Scripts\\activate"],"metadata":{"id":"5eEKogA4b4fi"}},{"cell_type":"code","source":["%pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\" chromadb\n"],"metadata":{"id":"dsJfSqgZ9qCv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!pip show autogen-agentchat"],"metadata":{"id":"ocn2CHBDGono"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OpenAI API Key.\n","\n","# For Google Colab environment.\n","from google.colab import userdata\n","key = userdata.get('OPENAI_API_KEY')\n","\n","# For local environment.\n","#import os\n","#\n","#key = os.getenv(\"OPENAI_API_KEY\")\n","\n","if not key:\n","    raise ValueError(\"API key not found. Please set the MY_API_KEY environment variable.\")\n","\n","print(\"API Key loaded successfully!\")"],"metadata":{"id":"fcz9QqGG9aLv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Simple RAG index with Chroma\n","\n","This is a tiny, self-contained RAG backend: load files from ./data, chunk text, store in Chroma, and expose a rag_search() function."],"metadata":{"id":"SiilvVffGawI"}},{"cell_type":"code","source":["import os\n","import glob\n","import textwrap\n","from typing import List\n","\n","import chromadb\n","from chromadb.utils import embedding_functions"],"metadata":{"id":"yM8i2gGN9Z61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------- 1) Create a Chroma client & embedding function ----------\n","chroma_client = chromadb.Client()\n","\n","embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n","    #api_key=os.environ[\"OPENAI_API_KEY\"],\n","    api_key=key,\n","    model_name=\"text-embedding-3-small\",\n",")\n","\n","COLLECTION_NAME = \"demo_docs\"\n","collection = None  # will be created inside index_docs"],"metadata":{"id":"QELF_T_s9Z35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------- 2) Helper: read all text-like files ----------\n","def read_file(path: str) -> str:\n","    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n","        return f.read()\n","\n","\n","def get_all_files(data_dir: str = \"./data\") -> List[str]:\n","    exts = (\"*.txt\", \"*.md\")\n","    files = []\n","    for ext in exts:\n","        files.extend(glob.glob(os.path.join(data_dir, ext)))\n","    return files"],"metadata":{"id":"hDL_0Ejh9Z1P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Chunk sizes that are too large or too small in RAG systems lead to poor context matches during retrieval.\n","- ***Large chunks*** mix multiple ideas or irrelevant details, diluting semantic embeddings and causing \"noise\" that confuses vector similarity searches\n","- ***Small chunks***, however, often split related information across fragments, losing necessary context for queries needing broader understanding and resulting in missed relevant spans."],"metadata":{"id":"Qt0YbbZYP2G2"}},{"cell_type":"code","source":["# ---------- 3) SAFE chunking implementation ----------\n","def chunk_text(text: str, max_chars: int = 800, overlap: int = 120) -> List[str]:\n","    \"\"\"\n","    Split text into overlapping chunks, without infinite loops.\n","    - If text shorter than max_chars: return single chunk.\n","    - Otherwise: advance forward; for the last chunk, we stop when end == n.\n","    \"\"\"\n","    n = len(text)\n","    if n <= max_chars:\n","        return [text]\n","\n","    chunks = []\n","    start = 0\n","\n","    while start < n:\n","        end = min(start + max_chars, n)\n","        chunks.append(text[start:end])\n","\n","        if end == n:\n","            break  # reached the end safely\n","\n","        # move forward with overlap\n","        start = max(0, end - overlap)\n","\n","    return chunks"],"metadata":{"id":"mQqRDwrG9Zx6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------- 4) Index docs into a fresh collection ----------\n","def index_docs(data_dir: str = \"./data\"):\n","    global collection\n","\n","    files = get_all_files(data_dir)\n","    if not files:\n","        raise RuntimeError(f\"No .txt/.md files found in {data_dir}\")\n","\n","    # Always recreate the collection for this demo\n","    try:\n","        chroma_client.delete_collection(COLLECTION_NAME)\n","    except Exception:\n","        pass  # it's fine if it didn't exist yet\n","\n","    collection = chroma_client.create_collection(\n","        name=COLLECTION_NAME,\n","        embedding_function=embedding_fn,\n","    )\n","\n","    doc_ids = []\n","    texts = []\n","    metadatas = []\n","\n","    for path in files:\n","        print(f\"Processing {path}\")\n","        raw = read_file(path)\n","        chunks = chunk_text(raw, max_chars=800, overlap=120)\n","\n","        for i, ch in enumerate(chunks):\n","            doc_ids.append(f\"{os.path.basename(path)}__{i}\")\n","            texts.append(ch)\n","            metadatas.append({\"source\": os.path.basename(path), \"chunk\": i})\n","\n","    collection.add(ids=doc_ids, documents=texts, metadatas=metadatas)\n","    print(f\"Indexed {len(texts)} chunks from {len(files)} files.\")\n","\n","\n","# Call this once after your docs are in ./data\n","index_docs(\"./data\")"],"metadata":{"id":"K8gdId7-9Zu8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define a RAG tool (rag_search) for the agent"],"metadata":{"id":"rIzNzyYtIgDl"}},{"cell_type":"code","source":["async def rag_search(query: str, top_k: int = 4) -> str:\n","    \"\"\"\n","    Tool: search internal docs for relevant snippets.\n","    Returns a concatenated context string the model can use.\n","    \"\"\"\n","    results = collection.query(\n","        query_texts=[query],\n","        n_results=top_k,\n","    )\n","\n","    docs = results.get(\"documents\", [[]])[0]\n","    metas = results.get(\"metadatas\", [[]])[0]\n","\n","    if not docs:\n","        return \"No relevant context found in the knowledge base.\"\n","\n","    formatted_chunks = []\n","    for doc, meta in zip(docs, metas):\n","        # source identifies where the document chunk came from (e.g., filename, URL).\n","        # chunk identifies the chunk number or segment ID.\n","        # If the metadata doesn't contain those keys, defaults are used:\n","        #   - \"unknown\" for source\n","        #   - 0 for chunk_id\n","        src = meta.get(\"source\", \"unknown\")\n","        chunk_id = meta.get(\"chunk\", 0)\n","\n","        # Removes newlines from doc,\n","        # Shortens it to a maximum width of 400 characters,\n","        # If shortened, adds \" ...\" at the end\n","        snippet = textwrap.shorten(doc.replace(\"\\n\", \" \"), width=400, placeholder=\" ...\")\n","        formatted_chunks.append(f\"[{src}#{chunk_id}] {snippet}\")\n","\n","    return \"\\n\".join(formatted_chunks)\n"],"metadata":{"id":"9dfqkMt39Zrt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create the RAG assistant (new autogen_agentchat style)"],"metadata":{"id":"pla1MaV0IwEs"}},{"cell_type":"code","source":["import asyncio\n","from autogen_agentchat.agents import AssistantAgent\n","from autogen_agentchat.messages import TextMessage\n","from autogen_ext.models.openai import OpenAIChatCompletionClient\n"],"metadata":{"id":"wSBLIXzXIh6a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------- 1) Model client ----------\n","model_client = OpenAIChatCompletionClient(\n","    model=\"gpt-4.1-mini\",   # or gpt-4o, gpt-4.1, etc.\n","    api_key=key,\n",")\n","\n","# ---------- 2) RAG Answering Agent ----------\n","rag_agent = AssistantAgent(\n","    name=\"rag_assistant\",\n","    model_client=model_client,\n","    system_message=(\n","        \"You are a Q&A assistant over our internal security documentation.\\n\"\n","        \"- Use the `rag_search` tool to fetch relevant context.\\n\"\n","        \"- Ground your answers ONLY in that context.\\n\"\n","        \"- If the answer is not in context, say you don't know.\\n\"\n","    ),\n","    tools=[rag_search],          # <-- our RAG tool\n","    max_tool_iterations=2,       # let it call the tool at most twice\n",")\n"],"metadata":{"id":"yh4AGCbMIh3f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Helper to pull out the final answer text."],"metadata":{"id":"c7uE7H3sJIbV"}},{"cell_type":"code","source":["from autogen_agentchat.base import TaskResult\n","\n","def extract_last_text(result: TaskResult) -> str:\n","    # Find the last TextMessage in the task result\n","    for msg in reversed(result.messages):\n","        if isinstance(msg, TextMessage):\n","            return msg.content\n","    return \"\"\n"],"metadata":{"id":"iusvZSnPIh0U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Optional second agent: Writer / Refiner."],"metadata":{"id":"NUioKggxJNDH"}},{"cell_type":"code","source":["writer_agent = AssistantAgent(\n","    name=\"writer_agent\",\n","    model_client=model_client,\n","    system_message=(\n","        \"You are a senior technical writer.\\n\"\n","        \"Rewrite answers for the specified audience in clear, simple language.\\n\"\n","        \"Keep it concise and structured with bullets where useful.\"\n","    ),\n",")\n"],"metadata":{"id":"q7Hy6F77Ihxk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Refinement helper."],"metadata":{"id":"TCPMDh9sJR8e"}},{"cell_type":"code","source":["async def refine_for_audience(raw_answer: str,\n","                              audience: str = \"non-technical business stakeholder\",\n","                              max_words: int = 200) -> str:\n","    prompt = f\"\"\"\n","    Here is an AI-generated answer:\n","\n","    ---\n","    {raw_answer}\n","    ---\n","\n","    TASK:\n","    1. Rewrite this for the audience: {audience}.\n","    2. Use clear, plain language and avoid heavy jargon.\n","    3. Keep it under {max_words} words.\n","    4. Use bullets or short paragraphs for readability.\n","    \"\"\"\n","\n","    result = await writer_agent.run(task=textwrap.dedent(prompt).strip())\n","    return extract_last_text(result)\n"],"metadata":{"id":"WldjT-G9Ihuc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Full end-to-end Agentic RAG flow\n","\n","This is your agentic pipeline without RetrieveUserProxyAgent:\n","\n","1. rag_agent uses a RAG tool (rag_search) to get context from Chroma.\n","\n","2. It generates a grounded answer.\n","\n","3. writer_agent rewrites the answer for a particular audience."],"metadata":{"id":"bmxxrcMiJb5d"}},{"cell_type":"code","source":["async def rag_then_refine(question: str):\n","    print(\"\\n\" + \"#\" * 80)\n","    print(\"QUESTION:\", question)\n","    print(\"#\" * 80)\n","\n","    # 1) Ask the RAG assistant\n","    task = (\n","        \"Answer the following question using ONLY the internal docs. \"\n","        \"ALWAYS call the `rag_search` tool first to fetch context.\\n\\n\"\n","        f\"Question: {question}\"\n","    )\n","    rag_result = await rag_agent.run(task=task)\n","    raw_answer = extract_last_text(rag_result)\n","\n","    print(\"\\n--- RAW RAG ANSWER ---\\n\")\n","    print(raw_answer)\n","\n","    # 2) Refine for executives\n","    refined = await refine_for_audience(\n","        raw_answer,\n","        audience=\"C-level executive with limited technical background\",\n","        max_words=180,\n","    )\n","\n","    print(\"\\n--- REFINED FOR EXECUTIVES ---\\n\")\n","    print(refined)\n","    print(\"\\n\" + \"#\" * 80 + \"\\n\")\n","\n","    return raw_answer, refined\n","\n","\n","# Run one or two demo questions\n","async def main():\n","    await rag_then_refine(\"What are the main steps in our security review process?\")\n","    await rag_then_refine(\"Summarize the key benefits of our security policy.\")\n","\n","await main()\n"],"metadata":{"id":"CR7evlwcIhrK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CqsBd9xWIhn1"},"execution_count":null,"outputs":[]}]}